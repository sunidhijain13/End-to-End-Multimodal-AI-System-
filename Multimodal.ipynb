{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "960e08b9-b644-41fc-b40b-4d9120bf32ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74b6afb3-0b69-4474-8fd2-4605efef6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt_model.to(device)\n",
    "\n",
    "# pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load an image from a URL\n",
    "def load_image(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img\n",
    "\n",
    "image_url = \"https://assets.aboutamazon.com/7a/f6/2381a7084f3184ed19fc33d2efae/taylor-swift-eras-ext-pvod-1920x1080.jpeg\"\n",
    "image = load_image(image_url)\n",
    "\n",
    "# Preprocess the image for CLIP\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63a71f37-089c-4036-8f31-82369eb6b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Description:  a photo of Taylor Swift performing at a concert\n",
      "Generated Caption:  A concert photo of Taylor Swift performing on stage.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Text descriptions\n",
    "text_descriptions = [\n",
    "    \"a concert photo of a famous singer\",\n",
    "    \"a live performance on stage\",\n",
    "    \"a famous singer performing at a concert\",\n",
    "    \"a photo of Taylor Swift performing at a concert\",\n",
    "    \"a crowd watching a concert\",\n",
    "    \"a stage performance with bright lights\",\n",
    "    \"a singer performing in front of a huge audience\"\n",
    "]\n",
    "\n",
    "# Tokenize the descriptions\n",
    "text_inputs = clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "# Calculate similarities between the image and descriptions\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compare image features with text features\n",
    "    similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Find the best matching description\n",
    "best_match_index = similarities.argmax().item()\n",
    "best_description = text_descriptions[best_match_index]\n",
    "print(\"Best Description: \", best_description)\n",
    "\n",
    "def clean_caption(caption):\n",
    "    # Remove irrelevant parts if they exist\n",
    "    stop_phrases = ['Getty Images', 'Oscar', 'The Artist', 'Photo', '(']\n",
    "    for phrase in stop_phrases:\n",
    "        if phrase in caption:\n",
    "            caption = caption.split(phrase)[0].strip()\n",
    "    return caption\n",
    "\n",
    "prompt = f\"A concert photo of Taylor Swift performing on stage.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "output_sequences = gpt_model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'], \n",
    "    max_length=30,  \n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and post-process the output to remove nonsense\n",
    "caption = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "# Clean caption to remove irrelevant parts\n",
    "cleaned_caption = clean_caption(caption)\n",
    "print(\"Generated Caption: \", cleaned_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29418086-fd6d-498d-a351-fddf7ac179f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
